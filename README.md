# Containerized batch pipeline using DockerHub,Pachyderm, Google Cloud Storage,Postgres Cloud Database
## About this Repository
This repository contains the pipeline specifications and code for making batch pipelines for the larger [Hugging Face Transformers] (https://huggingface.co/models) based Question Answer model.
The goal of this repository is to connect DockerHub through Github actions in order to create our pipelines in Dockerhub. We will be creating 2 separate pipelines. These pipelines will further interact with our terminal (Cloud Shell) with the use of Pachyderm.
Each of these pipelines will have a python file, Dockerfile, and json file. Through Github actions, these pipelines will build images and finally push these images to Docker Hub. Once both these pipelines are deployed with the use of Pachyderm on Cloud Shell from GCS, with the use of PostgreSQL (2nd pipeline), they will eventually start running on Google Cloud Run.

## Batch Pipelines
The REST API for which these batch pipelines are constructed is an API made to provide the user the ability to ask questions based on provided contexts, by uploading them as .csv files, and the API generates the answer. The goal of these batch pipelines is to act as data pipelines between the REST API and the Cloud SQL database, where the answers generated by the pipeline is stored. The pipelines automate the data storage process by acting as a non-real time data updation link between the model and the database.

### Pipeline 1: Pulls the data uploaded by a user in a google cloud storage in the form of CSVs. 
There CSVs contain a list of questions and contexts. It then processes the questions asked to fetch ansers using a defualt BERT model for question answering.
Answers fetched are then written to an output directory.

### Pipeline 2: Pushes the answers created by first pipeline to a PostGres database on google cloud. 
The goal of these batch pipelines is to act as data pipelines between the Cloud storage and the 
Cloud SQL database, where the answers generated by the pipeline is stored.
The pipelines automate the data storage process by acting as a non-real time data updation link between the model and the database. 

Below is a high-level diagram to show how the pipelines fit in the overall framework:</br>


<img width="889" alt="3" src="https://user-images.githubusercontent.com/84465734/121768365-e02cd980-cb2b-11eb-9a6e-7776ff856d55.PNG">

### Pipeline 1 Operation
Through the pipeline, we would be able to upload multiple .csv files directly as an input, processes them using the available Hugging Face models and generates the respective answers as an output in Docker Hub. The answers.csv file will be generated with the outputs in Docker Hub.

### Pipeline 2 Operation
The 2nd Pipeline would connect to our Cloud SQL through Postgre SQL. The answers file generated will be the input here.

## Deploying these pipelines
### Prerequisites 
- Must have a valid account on dockerhub - 
- Must have a google cloud storage already set up 
- Must have access to pachyderm - https://hub.pachyderm.com/landing

## Deployment using Pachyderm Hub
Here we have used Cloud Shell terminal to deploy our pipelines using Pachyderm. Following are the steps to be following for deployment:


###  Build and deploy the docker images to DockerHub
- Add secret DOCKERHUB_USERNAME = <Your dockerhub username> in github secrets
- Add secret DOCKERHUB_TOKEN = <Your dockerhub access> in github secrets 
- Change your <your-dockerhub-username>/<docker-imaeg-name> in  .github/workflows/main.yml

        - - name: Build and push
      run: |-
        cd pachyderm_01 && docker build -t <your-dockerhub-username>/<docker-imaeg-name>:${{  github.sha }} .
        docker push <your-dockerhub-username>/<docker-imaeg-name>:${{  github.sha }} && cd ../
        cd pachyderm_02 && docker build -t <your-dockerhub-username>/<docker-imaeg-name>:${{  github.sha }} .
        docker push <your-dockerhub-username>/<docker-imaeg-name>:${{  github.sha }}

###  Create pachyderm workspace
####  Sign-in to Pachyderm
#### https://hub.pachyderm.com/landing
####  Create a new workspace as shown below
   <img width="309" alt="1" src="https://user-images.githubusercontent.com/84465734/121768778-fb98e400-cb2d-11eb-91c4-46e1946636f2.PNG">

 Refer : https://docs.pachyderm.com/latest/hub/hub_getting_started/

####  Login to Pachyderm Cluster 


####  Getting Started with Pachyderm Hub

**Create a 4 hr workspace if you're using the free version of pachctl**
Note that the workspace will disappear after 4 hours and a new workspace would need to be created and loaded on the terminal
**Install pachyderm**
- Run the corresponding steps for your operating system on your terminal
For macOS, run:
```
brew tap pachyderm/tap && brew install pachyderm/tap/pachctl@1.13
```
For a Debian-based Linux 64-bit or Windows 10 or later running on WSL/Cloud Shell:
```
curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v1.13.2/pachctl_1.13.2_amd64.deb && sudo dpkg -i /tmp/pachctl.deb
```
For all other Linux flavors:
```
curl -o /tmp/pachctl.tar.gz -L https://github.com/pachyderm/pachyderm/releases/download/v1.13.2/pac
```

####  Connect to your Pachyderm workspace
Click "Connect" on your Pachyderm workspace and follow the below listed steps to connect to your workspace via the machine:
   
   <img width="306" alt="2" src="https://user-images.githubusercontent.com/84465734/121769024-50892a00-cb2f-11eb-8546-97b039618abb.PNG">

**Connect Pachyderm Workspace with Cloud Shell**
To configure a Pachyderm context and log in to your workspace (i.e. have your pachctl point to your new workspace), click the Connect link on your workspace name in the Hub UI.


#### Create your secret on Google Cloud
Go to IAM & Admin --> Service Accounts --> Creat Service Account --> Input Roles
**Roles:**
Storage Admin
Storage Object Creator
Storage Object Viewer
Click on 'Create' Service Account.
Go to Keys, and generate your secret. A jsn file will be downloaded for you, save that file

####  Upload secret created
Upload the json file on Cloud Shell
Following the following steps
```
chmod +x create_secret.sh
```
Add your json secret name generated here
```
export GOOGLE_APPLICATION_CREDENTIALS="json.file"
```
Create your secret
```
./create_secret.sh
```

####  Verify whether secrets are created or not 
  ```
    pachctl list secret
```

#### Create your service on Docker Hub and built it
Go to Docker Hub and create a new service(check sir's video to understand)
Run the following command on your terminal
```
docker build -t singhals912/mgmt590-gcs
```
If your docker account is not logged in then type the following code and input your username and password
```
docker login
```

#### Create pipeline through pachctl
Run the following code on your terminal
```
pachctl create pipeline -f spec.json
```
Check the status of your pipeline using the command
```
watch pachctl list pipeline
```
Once your status updates, run the command to verify your status
```
pachctl list job
```
####  Input your encoded secret into GCS_CREDS in the restapi repo
To encode your secret input the following code:
```
base64 - w 0 singhals912-d747ead3f6b1.json
```
Input your secret name in " singhals912-d747ead3f6b1.json"
Copy this secret and put the text in your secrets in this repo as GCS_CREDS. Change the '=' to '@'

#### Make changes to Github and run-on Postman

Check your github actions and deploy your code on google cloud. Once that's done input your link on postman as:
```
https://assignment2-isd4ai77qq-uc.a.run.app/upload
```
Input any csv file in Body--> form-data. Put the Key as 'file' and send the command. 

#### Create another Docker repo
Create a new docker repo as mgmt590-sql in Dockerhub. The same name should be updated on the yaml file.
The code will automatically connect your output from pipeline 1 as the input for pipeline 2. The input will run in Cloud SQL through postgreSQL and return the output

### Your 2nd pipeline is created as well

## Expected Output from pachctl commands

```
singhals912@cloudshell:~/gcs-example (container-assgn2)$ pachctl list job ID PIPELINE STARTED DURATION RESTART PROGRESS DL UL STATE ea47124578294ea88c42234331a2c123 getfiles2 2 minutes ago 17 seconds 0 1 + 0 / 1 0B 1.773KiB success 
```

```
singhals912@cloudshell:~/gcstest2 (singhals912)$ pachctl list repo
NAME CREATED SIZE (MASTER) ACCESS LEVEL
getfiles2 7 minutes ago 1.773KiB OWNER Output repo for pipeline getfiles2.
getfiles2_tick 7 minutes ago 0B OWNER Cron tick repo for pipeline getfiles2.
```
``` 
singhals912@cloudshell:~/gcs-example (container-assgn2)$ pachctl list file getfiles2@87d05708c6444776bda86da1b9ef5c0a NAME TYPE SIZE /answers.csv file 3.547KiB
 
```

![image](https://user-images.githubusercontent.com/20911800/121794401-bd56ff80-cbd5-11eb-830e-1994bbb8c4e8.png)
